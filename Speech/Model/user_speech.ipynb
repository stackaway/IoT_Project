{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_document = '/Users/churnika/Desktop/Projects/IoT_Project/Speech/Dataset/speech_to_text_20240414-185854.csv'\n",
    "\n",
    "# Try to read the CSV file without headers\n",
    "try:\n",
    "    test_doc = pd.read_csv(test_document, header=None)\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"No data in {test_document}\")\n",
    "    test_doc = pd.DataFrame()\n",
    "\n",
    "# If the DataFrame is not empty, rename the column and convert to lowercase\n",
    "if not test_doc.empty:\n",
    "    test_doc.columns = ['text']\n",
    "    test_doc['text'] = test_doc['text'].str.lower()\n",
    "else:\n",
    "    print(\"The DataFrame is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# If you haven't downloaded the tokenizer package, uncomment the line below to download\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "if not test_doc.empty:\n",
    "    test_doc['tokenized_text'] = test_doc['text'].apply(tokenize_text)\n",
    "else:\n",
    "    print(\"The DataFrame is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [10, 5, 11, 1, 12, 13, 3, 2, 6, 7, 14, 1, 15, ...\n",
      "1    [20, 1, 21, 2, 22, 8, 23, 5, 2, 1, 9, 2, 24, 2...\n",
      "2    [29, 30, 31, 3, 32, 33, 34, 2, 35, 3, 3, 36, 3...\n",
      "3    [45, 46, 47, 48, 49, 50, 1, 51, 52, 4, 53, 54,...\n",
      "Name: encoded_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the text\n",
    "# This will create the vocabulary\n",
    "if not test_doc.empty:\n",
    "    tokenizer.fit_on_texts(test_doc['tokenized_text'].tolist())\n",
    "\n",
    "# Convert the tokens into integers\n",
    "test_doc['encoded_text'] = test_doc['tokenized_text'].apply(lambda x: tokenizer.texts_to_sequences([x])[0])\n",
    "\n",
    "print(test_doc['encoded_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [10, 5, 11, 1, 12, 13, 3, 2, 6, 7, 14, 1, 15, ...\n",
      "1    [0, 0, 0, 0, 0, 20, 1, 21, 2, 22, 8, 23, 5, 2,...\n",
      "2    [0, 29, 30, 31, 3, 32, 33, 34, 2, 35, 3, 3, 36...\n",
      "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 46, 47, 48, 49...\n",
      "Name: padded_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the sequences\n",
    "# This will make all sequences the same length\n",
    "if not test_doc.empty:\n",
    "    test_doc['padded_text'] = pad_sequences(test_doc['encoded_text'].tolist()).tolist()\n",
    "\n",
    "print(test_doc['padded_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "[[1.97946490e-03 9.97500300e-01 4.11508692e-04 4.07480256e-05\n",
      "  6.75493648e-05 4.31243166e-07]\n",
      " [4.48219199e-03 9.94602323e-01 2.64460163e-04 2.94513622e-04\n",
      "  3.55155877e-04 1.43886280e-06]\n",
      " [5.46608627e-01 1.03738934e-01 8.61994829e-03 2.28097931e-01\n",
      "  1.04481518e-01 8.45306646e-03]\n",
      " [1.36429563e-01 4.31507230e-01 7.72893578e-02 8.75468627e-02\n",
      "  2.06835806e-01 6.03912286e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# Convert the 'padded_text' column to a numpy array\n",
    "X = np.array(test_doc['padded_text'].tolist())\n",
    "\n",
    "# Use the model to make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy', 'joy', 'sadness', 'joy']\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary to map indices to class names\n",
    "index_to_class = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear'\n",
    "}\n",
    "\n",
    "# Find the index of the maximum probability for each sequence\n",
    "max_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map the indices to class names\n",
    "predicted_classes = [index_to_class[index] for index in max_indices]\n",
    "\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the occurrences of each class\n",
    "counter = Counter(predicted_classes)\n",
    "\n",
    "# Find the class with the highest count\n",
    "overall_class = counter.most_common(1)[0][0]\n",
    "\n",
    "print(overall_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "import ssl\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "# Check if the overall class is 'sadness'\n",
    "    # Email settings\n",
    "subject = \"An important message\"\n",
    "body = \"Please find the attached CSV file for more details on your patient.\"\n",
    "sender_email = \"\"\n",
    "receiver_email = \"\"\n",
    "password = \"\"\n",
    "filename = \"/Users/churnika/Desktop/Projects/IoT_Project/Speech/Dataset/speech_to_text_20240414-142942.csv\"\n",
    "# Create a multipart message\n",
    "msg = MIMEMultipart()\n",
    "msg[\"From\"] = sender_email\n",
    "msg[\"To\"] = receiver_email\n",
    "msg[\"Subject\"] = subject\n",
    "# Add the email body\n",
    "msg.attach(MIMEText(body, \"plain\"))\n",
    "# Open the file in binary mode\n",
    "with open(filename, \"rb\") as attachment:\n",
    "    # Add file as application/octet-stream\n",
    "    part = MIMEBase(\"application\", \"octet-stream\")\n",
    "    part.set_payload(attachment.read())\n",
    "# Encode file in ASCII characters to send by email    \n",
    "encoders.encode_base64(part)\n",
    "# Add header as pdf attachment\n",
    "part.add_header(\n",
    "    \"Content-Disposition\",\n",
    "    f\"attachment; filename= {filename}\",\n",
    ")\n",
    "# Add attachment to message and convert message to string\n",
    "msg.attach(part)\n",
    "text = msg.as_string()\n",
    "# Log in to server using secure context and send email\n",
    "context = ssl.create_default_context()\n",
    "with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(sender_email, receiver_email, text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
